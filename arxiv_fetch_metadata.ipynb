{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "* Fetch metadata from arvix file\n",
    "* By: Neel Shah\n",
    "* Date: 4/3/2017\n",
    "* contact: neelknightme@gmail.com\n",
    "* Note: If you have any query, please mail me. And feedback are always welcome.\n",
    "*       It is available for anyone and modify it.\n",
    "*-------Data is displyed only: You have to add code to store data as you want----------\n",
    "'''\n",
    "\n",
    "import urllib\n",
    "import feedparser\n",
    "\n",
    "# Base api query url\n",
    "base_url = 'http://export.arxiv.org/api/query?';\n",
    "\n",
    "# Search parameters\n",
    "search_query = 'all:electron' # search for electron in all fields\n",
    "start = 0                     # retreive the first 5 results\n",
    "max_results = 5\n",
    "\n",
    "query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
    "                                                     start,\n",
    "                                                     max_results)\n",
    "\n",
    "# Opensearch metadata such as totalResults, startIndex, \n",
    "# and itemsPerPage live in the opensearch namespase.\n",
    "# Some entry metadata lives in the arXiv namespace.\n",
    "# This is a hack to expose both of these namespaces in\n",
    "# feedparser v4.1\n",
    "feedparser._FeedParserMixin.namespaces['http://a9.com/-/spec/opensearch/1.1/'] = 'opensearch'\n",
    "feedparser._FeedParserMixin.namespaces['http://arxiv.org/schemas/atom'] = 'arxiv'\n",
    "\n",
    "# perform a GET request using the base_url and query\n",
    "response = urllib.urlopen(base_url+query).read()\n",
    "\n",
    "# parse the response using feedparser\n",
    "feed = feedparser.parse(response)\n",
    "\n",
    "# print out feed information\n",
    "print 'Feed title: %s' % feed.feed.title\n",
    "print 'Feed last updated: %s' % feed.feed.updated\n",
    "\n",
    "# print opensearch metadata\n",
    "print 'totalResults for this query: %s' % feed.feed.opensearch_totalresults\n",
    "print 'itemsPerPage for this query: %s' % feed.feed.opensearch_itemsperpage\n",
    "print 'startIndex for this query: %s'   % feed.feed.opensearch_startindex\n",
    "\n",
    "# Run through each entry, and print out information\n",
    "for entry in feed.entries:\n",
    "    print 'e-print metadata'\n",
    "    print 'arxiv-id: %s' % entry.id.split('/abs/')[-1]\n",
    "    print 'Published: %s' % entry.published\n",
    "    print 'Title:  %s' % entry.title\n",
    "    \n",
    "    # feedparser v4.1 only grabs the first author\n",
    "    author_string = entry.author\n",
    "    \n",
    "    # grab the affiliation in <arxiv:affiliation> if present\n",
    "    # - this will only grab the first affiliation encountered\n",
    "    #   (the first affiliation for the first author)\n",
    "    try:\n",
    "        author_string += ' (%s)' % entry.arxiv_affiliation\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    print 'Last Author:  %s' % author_string\n",
    "    \n",
    "    # feedparser v5.0.1 correctly handles multiple authors, print them all\n",
    "    try:\n",
    "        print 'Authors:  %s' % ', '.join(author.name for author in entry.authors)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    # get the links to the abs page and pdf for this e-print\n",
    "    for link in entry.links:\n",
    "        if link.rel == 'alternate':\n",
    "            print 'abs page link: %s' % link.href\n",
    "        elif link.title == 'pdf':\n",
    "            print 'pdf link: %s' % link.href\n",
    "    \n",
    "    # The journal reference, comments and primary_category sections live under \n",
    "    # the arxiv namespace\n",
    "    try:\n",
    "        journal_ref = entry.arxiv_journal_ref\n",
    "    except AttributeError:\n",
    "        journal_ref = 'No journal ref found'\n",
    "    print 'Journal reference: %s' % journal_ref\n",
    "    \n",
    "    try:\n",
    "        comment = entry.arxiv_comment\n",
    "    except AttributeError:\n",
    "        comment = 'No comment found'\n",
    "    print 'Comments: %s' % comment\n",
    "    \n",
    "    # Since the <arxiv:primary_category> element has no data, only\n",
    "    # attributes, feedparser does not store anything inside\n",
    "    # entry.arxiv_primary_category\n",
    "    # This is a dirty hack to get the primary_category, just take the\n",
    "    # first element in entry.tags.  If anyone knows a better way to do\n",
    "    # this, please email the list!\n",
    "    print 'Primary Category: %s' % entry.tags[0]['term']\n",
    "    \n",
    "    # get all the categories\n",
    "    all_categories = [t['term'] for t in entry.tags]\n",
    "    print 'All Categories: %s' % (', ').join(all_categories)\n",
    "    \n",
    "    # The abstract is in the <summary> element\n",
    "    print 'Abstract: %s' %  entry.summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
